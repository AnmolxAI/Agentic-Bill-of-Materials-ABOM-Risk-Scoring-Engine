Technical AI Governance via an Agentic Bill of Materials and Risk Tiering

Introduction

The rapid emergence of agentic AI systems – AI agents with autonomy, tool use, memory, and self-directed goal pursuit – is reshaping the AI risk landscape[1][2]. Traditional AI governance focused on model accuracy and content outputs is insufficient when AI agents can act on the world (provision infrastructure, execute transactions, or control physical systems) rather than merely generate text[3]. International regulators and industry groups have recognized the need for new frameworks addressing these active operators as opposed to passive models[4][5]. The EU’s risk-based AI Act exemplifies this shift, introducing systemic risk categories for powerful general-purpose AI and mandating detailed technical documentation and transparency measures[6]. In parallel, leading AI labs have voluntarily published “frontier AI” safety policies that define capability thresholds and safety protocols for advanced models[7][8]. These developments signal a convergence on risk-tiered governance for high-agency AI.

This paper proposes a comprehensive technical solution for international AI governance tailored to agentic AI. We introduce an Agentic Bill of Materials (ABOM) – a standardized, machine-readable schema documenting an AI agent’s core components, autonomy settings, tool integrations, and memory mechanisms. Building on the EU’s Model Documentation Form for AI transparency[9][10], the ABOM extends it to capture features unique to agentic systems (like tool APIs, human oversight mode, and persistent state). Second, we define a Scoring Engine that computes a risk magnitude R from the ABOM’s key vectors: Agency (A) – the breadth/criticality of tools accessible; Autonomy (U) – the level of human oversight (from human-in-the-loop to out-of-the-loop); and Persistence (P) – the agent’s memory duration and state retention. The risk function is deliberately nonlinear (expensive growth in R for long-term memory) to reflect the compounding hazards of an agent that can strategize over extended time[11]. Third, we propose a Unified Agentic Risk Tiering (UART) framework with 5 tiers (0 through 4) mapping quantified risk to escalating control requirements. These tiers align with the EU AI Act’s risk hierarchy – e.g. Tier 4 agents correspond to “GPAI with systemic risk” as defined by exceeding certain compute/capability thresholds[6]. Finally, we outline implementation and verification mechanisms to enforce this governance model: the use of standardized tool APIs (Model Context Protocol), fine-grained capability tokens for agent actions, and hardware-based attestation (TPM, secure enclaves) for tamper-proof logs. Our approach enables “verification without trust,” allowing regulators and stakeholders to cryptographically verify an agent’s declared capabilities and compliance in situ, rather than relying on mere promises. The following sections detail each component of this framework and how, together, they meet the Technical AI Governance Challenge’s goals of enforceable international standards for AI safety and transparency.

Agentic Bill of Materials (ABOM)

Definition: The Agentic Bill of Materials (ABOM) is a structured documentation schema that enumerates all critical components and parameters of an agentic AI system. It serves as an analogue to a software bill of materials (SBOM), but for AI agency. The EU AI Act already requires providers of high-risk or general-purpose AI to prepare extensive technical documentation (covering system architecture, training data, performance metrics, etc.)[9][12]. The ABOM builds on this by specifically cataloguing the elements that determine an AI agent’s capability and risk profile. Each AI agent instance would have an ABOM file (in a standard JSON/YAML format) that includes:

Model Core: Details of the AI model underpinning the agent. This covers the model architecture type (e.g. transformer network, RNN), size (parameters), and training information such as total compute expended (FLOPs). In the EU transparency template, model providers must report training data and model info[13][14]; the ABOM extends this with a “compute and architecture” field. For example, an ABOM entry might state the model is a 70B-parameter transformer trained on 10^25 FLOPs – a significant number since EU policy designates models above 10^25 FLOPs as potentially systemic-risk[6]. Documenting the Model Core allows auditors to quickly identify if an agent is built on a frontier model and subject to heightened scrutiny or obligations.

Agency Profile: A list of the external tools, APIs, and system permissions the agent has, including the scope of actions allowed for each (“permissions”). Crucially, the ABOM distinguishes state-changing tools from read-only ones. State-changing tools are those that can alter the environment or perform irreversible actions (database write, code execution, financial transaction), whereas read-only tools are limited to information retrieval (search, database query without modification, etc.)[15][16]. Each tool entry in the ABOM includes its name, version or interface, permitted operations, and safety constraints (e.g. rate limits or approval requirements). This component is essentially an access control matrix for the AI agent[17]. By enumerating tool integrations and their permission levels, the ABOM enables governance of an agent’s “blast radius” – you can immediately see what systems it could impact. For instance, an Agentic CRM Assistant might list a read-only CRM database API and an email-sending capability (with constraint “draft mode only, human approval to actually send”). In contrast, a riskier fully-automated DevOps agent might have state-changing access to a cloud infrastructure API (to deploy or terminate servers). Including the Agency Profile in a standardized way supports compliance with Role-Based Access Control (RBAC) principles and provides a checklist for tool tiering governance (e.g. many organizations classify tools by impact level[15], and the ABOM would reflect such classifications).

Autonomy Leash: The mode of human oversight and intervention designed into the agent – essentially whether it is Human-in-the-Loop (HITL), Human-on-the-Loop (HOTL), or Human-out-of-the-Loop (HOOTL)[18]. This autonomy setting is a critical part of the agent’s profile. In HITL mode, the agent must obtain human approval for key decisions or before executing each action[19]. In HOTL, the agent operates autonomously but a human supervisor monitors its activity and can intervene on detection of anomalies[20]. In HOOTL, the agent operates fully independently within predefined bounds; humans set objectives and constraints but do not actively monitor routine operations[21]. The ABOM explicitly records which of these governance modes is active, as well as any autonomy constraints (e.g. an agent might be HOOTL for most tasks but require HITL for financial transactions above a certain size). By declaring the autonomy leash, the ABOM makes transparent the degree of human control enforced, aligning with EU AI Act provisions on human oversight[22]. Regulators can treat this as a declared safety mechanism: an agent marked HOOTL obviously demands stricter external oversight than one marked HITL.

Persistence Layer: Specifications of the agent’s memory and state retention mechanisms. Traditional LLM-based systems are stateless beyond a single prompt-response, but agentic systems may retain memory across sessions or continuously update an internal state (via databases, vector embeddings, scratchpad files, etc.)[23]. The ABOM describes what memory stores the agent uses and how long/stateful the agent is. Persistence is a double-edged sword: it improves an agent’s capability to perform long-horizon tasks, but it also introduces new risks like long-term memory poisoning[11]. Hence, the ABOM’s persistence section notes the retention duration, expiry policies, and data types stored.

Scaffolding & Orchestration: The ABOM lists the agent’s scaffolding – the orchestration framework or agent architecture that wraps the core model. Modern agentic systems often use frameworks or custom orchestrators that handle task planning, tool selection, and error recovery[26][27]. This section also documents sandboxing and isolation settings.

Figure 1. ABOM Structure Schema: Machine-readable hierarchical inventory of an agent’s architecture, autonomy, and scaffolding.

Figure 2: ABOM-to-Risk scoring pipeline. The Agentic Bill of Materials (ABOM) documents key attributes (tools, autonomy mode, memory, etc.). A scoring engine maps these to risk vectors A, U, P and computes a composite Risk Magnitude R. The risk score then determines the agent’s regulatory tier (UART level).


Risk Scoring Engine (A, U, P → R)

Given a filled-out ABOM for an AI agent, the next step is to quantify its risk magnitude R. We define a scoring engine that derives three primary risk vectors from the ABOM – Agency (A), Autonomy (U), Persistence (P) – and then computes a single scalar R via a formula. The motivation is to condense the many factors of an agent into a risk score that correlates with the potential for harm or misuse. This mirrors approaches in other fields (e.g. cybersecurity risk scores, or automotive safety integrity levels) but tailored to AI agency. Below we define A, U, P and propose a risk function:

Agency (A): This score reflects the breadth and criticality of tools/actions the agent can perform – effectively the potential “blast radius” of the agent if it behaves undesirably. An agent with no ability to change any system state (no tools or only read-only API access) would have A = 0 or minimal, whereas an agent that can execute shell commands, make financial trades, or alter physical equipment would have a high A. We can derive A from the ABOM’s Agency Profile: count of tools and an impact weight for each. For instance, one simple schema: assign 0 points for read-only tools, 1 point for low-impact writes (like drafting an email, which is reversible), 2 for moderate-impact operations (updating a database record or posting a message), and 3 for high-impact or irreversible actions (e.g. transferring money, deleting data, controlling an actuator)[32]. Sum or otherwise combine these to get A. Additionally, critical tools (those directly affecting safety or large-scale systems) could amplify A non-linearly. The breadth of accessible domains also matters – an agent that only operates in one constrained domain might be less risky than one plugged into many unrelated systems. However, for initial simplicity, we treat A as a composite score where higher means the agent can do more things and/or more dangerous things. This aligns with ISO/IEC 62443’s concept of zones and conduits: more conduits into critical systems means higher security level needed[33][34].

Autonomy (U): This score corresponds to the degree of human oversight (or lack thereof) on the agent’s actions, as documented by the ABOM’s Autonomy Leash. If the agent is Human-in-the-Loop for all high-impact actions, risk is significantly mitigated because a human gatekeeper can catch errors or malicious outputs[35]. By contrast, a Human-out-of-the-Loop agent has no builtin fail-safe from human judgment during operation – it could make rapid consequential decisions without a human in the decision chain[36]. We therefore assign U values in increasing order: U = 1 for HITL, U = 2 for HOTL, and U = 3 for HOOTL. This scheme is analogous to safety levels in autonomous vehicles (SAE J3016)[37][38].

Persistence (P): This score quantifies the agent’s memory lifespan and retention. Persistence is a force-multiplier for risk: an agent that forgets everything after each task has limited capacity to compound errors or subvert controls, whereas an agent that accumulates knowledge across sessions could develop more powerful strategies. P = 0 for strictly ephemeral memory, P = 1 for short-term memory, and P = 2 for long-term persistent memory. The risk function treats P exponentially[11].

Mitigation (M): This modifier reflects the presence of safety controls documented in the ABOM's Scaffolding & Orchestration section. M ranges from 0.7 to 1.0, where lower values indicate stronger mitigations. Factors that reduce M include: use of Model Context Protocol (MCP) for standardized tool invocation, sandboxed/isolated execution environments, and circuit breakers or kill-switches. An agent with all safety controls active may receive up to 30% risk reduction.

Risk Function:

R = A × U × e^P × M

The result R is mapped to discrete risk tiers. The exponential penalty for P reflects the compounding hazards of long-horizon autonomous behavior[39], while M rewards agents that implement robust safety scaffolding.

Unified Agentic Risk Tiering (UART)

We define a five-tier Unified Agentic Risk Tiering (UART) framework (Tier 0–4) aligned with the EU AI Act and analogous standards such as SAE J3016 and ISA/IEC 62443[37][40].

Tier 0 – Passive: No meaningful agency, no autonomous action, no state-changing tools. Comparable to standard chatbots. Minimal governance requirements.

Tier 1 – Assistive: Limited agency with Human-in-the-Loop enforcement. The agent may draft outputs or suggest actions but cannot execute high-impact actions without approval. Logging and transparency required.

Tier 2 – Bounded: Human-on-the-Loop agents operating autonomously within strict guardrails. Circuit breakers, sandboxing, and anomaly detection required. Corresponds to EU high-risk AI systems.

Tier 3 – High-Agency: Human-out-of-the-Loop agents with access to impactful tools. Requires continuous monitoring, watchdogs, tamper-evident logging, non-human identity binding, and periodic audits[43][44][47][48].

Tier 4 – Systemic: Fully autonomous, infrastructure-critical, or frontier agents posing systemic risk. Requires hardware-enforced governance (TPM, secure enclaves), immutable audit logs, kill-switches, regulator oversight, and continuous evaluation[53][54].

Figure 2. Unified Agentic Risk Tiering (UART).

Implementation and Verification Mechanisms

Model Context Protocol (MCP) standardizes tool invocation and enables policy enforcement at the interface layer[28]. Capability-based access using scoped tokens enforces least privilege and prevents unauthorized actions even if the agent misbehaves[72][73]. Hardware-based attestation (TPM, secure enclaves) enables cryptographic verification of agent configuration, execution integrity, and audit logs[82][83].

Secure execution environments allow startup and runtime attestation, while tamper-evident logging ensures accountability. Tier-based enforcement profiles automatically apply safeguards proportional to risk.

Conclusion

We presented a technical governance framework for agentic AI based on transparent documentation (ABOM), quantitative risk scoring, unified tiering (UART), and enforceable verification mechanisms. This approach aligns with the EU AI Act and international standards while enabling verification without trust. By embedding governance into the technical substrate of AI agents, this framework enables scalable, enforceable, and internationally harmonized AI safety.

References:
[Full references list exactly as in the original document]
